{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does not reflect definitive results of our study and is solely for ad hoc exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Do not run this notebook. It has been superseeded by a more systematic investigation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import read_json\n",
    "data = read_json(\"../../data/ort_human.json\")\n",
    "\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = list({line[\"source\"] for user_line in data for line in user_line[\"lines\"]})\n",
    "\n",
    "src_to_n1 = {line[\"source\"] : line[\"translations\"][\"4\"][\"orig\"] for user_line in data for line in user_line[\"lines\"]}\n",
    "src_to_p1 = {line[\"source\"] : line[\"translations\"][\"1\"][\"orig\"] for user_line in data for line in user_line[\"lines\"]}\n",
    "src_to_p3 = {line[\"source\"] : line[\"translations\"][\"3\"][\"orig\"] for user_line in data for line in user_line[\"lines\"]}\n",
    "src_to_n1_pe = {line[\"source\"]: line[\"translations\"][\"4\"][\"done\"] for user_line in data for line in user_line[\"lines\"]}\n",
    "src_to_p1_pe = {line[\"source\"]: line[\"translations\"][\"1\"][\"done\"] for user_line in data for line in user_line[\"lines\"]}\n",
    "src_to_p3_pe = {line[\"source\"]: line[\"translations\"][\"3\"][\"done\"] for user_line in data for line in user_line[\"lines\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtme -t wmt20 -l en-cs --scores > ~/optimal-reference-translations/data/data_tmp/scores.tsv\n",
    "# mtme -t wmt20 -l en-cs --echosys doc,src > ~/optimal-reference-translations/data/data_tmp/text.tsv\n",
    "# remove first line from mscores\n",
    "\n",
    "with open(\"../../data/data_tmp/text.tsv\", \"r\") as f:\n",
    "    src_to_i = {\n",
    "        l.removesuffix(\"\\n\").split(\"\\t\")[3]:i\n",
    "        for i, l in enumerate(f.readlines())\n",
    "    }\n",
    "\n",
    "with open(\"../../data/data_tmp/scores.tsv\", \"r\") as f:\n",
    "    i_to_score = {\n",
    "        i:float(l.split(\"\\t\")[5]) if l.split(\"\\t\")[5] != \"None\" else 0\n",
    "        for i, l in enumerate(f.readlines())\n",
    "    }\n",
    "\n",
    "with open(\"../../data/data_tmp/text.tsv\", \"r\") as f:\n",
    "    tgts_all = [\n",
    "        l.split(\"\\t\")[1]\n",
    "        for i, l in enumerate(f.readlines())\n",
    "    ]\n",
    "\n",
    "    tgts = [\n",
    "        tgts_all[src_to_i[s]]\n",
    "        for s in srcs\n",
    "    ]\n",
    "\n",
    "with open(\"../../data/data_tmp/text.tsv\", \"r\") as f:\n",
    "    tgt_to_score = {\n",
    "        l.split(\"\\t\")[1]:i_to_score[i]\n",
    "        for i, l in enumerate(f.readlines())\n",
    "    }\n",
    "\n",
    "human = [\n",
    "    tgt_to_score[tgt]\n",
    "    for tgt in tgts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(srcs, tgts, ref_map):\n",
    "    refs = [ref_map[line] for line in srcs]\n",
    "    batched = [\n",
    "        {\"ref\": line[2], \"src\": line[0], \"mt\": line[1]}\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    scores = model.predict(batched, gpus=0, batch_size=10)[\"scores\"]\n",
    "    return np.array(scores)\n",
    "\n",
    "comet_n1 = get_scores(srcs, tgts, src_to_n1)\n",
    "comet_p1 = get_scores(srcs, tgts, src_to_p1_pe)\n",
    "comet_p3 = get_scores(srcs, tgts, src_to_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "model_chrf = load(\"chrf\")\n",
    "\n",
    "def get_scores(srcs, tgts, ref_map):\n",
    "    refs = [ref_map[line] for line in srcs]\n",
    "    scores = [\n",
    "        model_chrf.compute(predictions=[line[1]], references=[[line[2]]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def get_scores_all(srcs, tgts, ref_maps):\n",
    "    refs = [[ref_map[line] for ref_map in ref_maps] for line in srcs]\n",
    "    scores = [\n",
    "        model_chrf.compute(predictions=[line[1]], references=[line[2]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "chrf_n1 = get_scores(srcs, tgts, src_to_n1)\n",
    "chrf_p1 = get_scores(srcs, tgts, src_to_p1_pe)\n",
    "chrf_p3 = get_scores(srcs, tgts, src_to_p3)\n",
    "chrf_n1_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1])\n",
    "chrf_n1s_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1_pe])\n",
    "chrf_n1_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3])\n",
    "chrf_n1_p1s_p3_n1s_p3s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3, src_to_n1_pe, src_to_p3_pe])\n",
    "chrf_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_p3])\n",
    "chrf_n1_p3 = get_scores_all(srcs, tgts, [src_to_p3, src_to_n1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "model_bleu = load(\"sacrebleu\")\n",
    "\n",
    "def get_scores(srcs, tgts, ref_map):\n",
    "    refs = [ref_map[line] for line in srcs]\n",
    "    scores = [\n",
    "        model_bleu.compute(predictions=[line[1]], references=[[line[2]]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "def get_scores_all(srcs, tgts, ref_maps):\n",
    "    refs = [[ref_map[line] for ref_map in ref_maps] for line in srcs]\n",
    "    scores = [\n",
    "        model_bleu.compute(predictions=[line[1]], references=[line[2]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "bleu_n1 = get_scores(srcs, tgts, src_to_n1)\n",
    "bleu_p1 = get_scores(srcs, tgts, src_to_p1_pe)\n",
    "bleu_p3 = get_scores(srcs, tgts, src_to_p3)\n",
    "bleu_n1_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1])\n",
    "bleu_n1s_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1_pe])\n",
    "bleu_n1_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3])\n",
    "bleu_n1_p1s_p3_n1s_p3s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3, src_to_n1_pe, src_to_p3_pe])\n",
    "bleu_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_p3])\n",
    "bleu_n1_p3 = get_scores_all(srcs, tgts, [src_to_p3, src_to_n1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "model_bleu = load(\"ter\")\n",
    "\n",
    "def get_scores(srcs, tgts, ref_map):\n",
    "    refs = [ref_map[line] for line in srcs]\n",
    "    scores = [\n",
    "        model_bleu.compute(predictions=[line[1]], references=[[line[2]]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "def get_scores_all(srcs, tgts, ref_maps):\n",
    "    refs = [[ref_map[line] for ref_map in ref_maps] for line in srcs]\n",
    "    scores = [\n",
    "        model_bleu.compute(predictions=[line[1]], references=[line[2]])[\"score\"]\n",
    "        for line in zip(srcs, tgts, refs)\n",
    "    ]\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "ter_n1 = get_scores(srcs, tgts, src_to_n1)\n",
    "ter_p1 = get_scores(srcs, tgts, src_to_p1_pe)\n",
    "ter_p3 = get_scores(srcs, tgts, src_to_p3)\n",
    "ter_n1_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1])\n",
    "ter_n1s_p1s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1_pe])\n",
    "ter_n1_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3])\n",
    "ter_n1_p1s_p3_n1s_p3s = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_n1, src_to_p3, src_to_n1_pe, src_to_p3_pe])\n",
    "ter_p1s_p3 = get_scores_all(srcs, tgts, [src_to_p1_pe, src_to_p3])\n",
    "ter_n1_p3 = get_scores_all(srcs, tgts, [src_to_p3, src_to_n1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def corr_nice(a, b):\n",
    "    return f\"{pearsonr(a, b)[0]:.2f}\"\n",
    "\n",
    "\n",
    "def print_table(source_txt, scores_comet, scores_bleu, scores_chrf, scores_ter, scores_human):\n",
    "    print(f\"Computed with {source_txt} as references\")\n",
    "    print(\n",
    "        \"Metric\", \"COMET-DA\", \"Human\",\n",
    "        sep=\" | \"\n",
    "    )\n",
    "    print(\n",
    "        \"-\", \"-\", \"-\",\n",
    "        sep=\"|\"\n",
    "    )\n",
    "    print(\n",
    "        \"chrF\",\n",
    "        corr_nice(scores_comet, scores_chrf),\n",
    "        corr_nice(scores_human, scores_chrf),\n",
    "        sep=\" | \"\n",
    "    )\n",
    "    print(\n",
    "        \"BLEU\",\n",
    "        corr_nice(scores_comet, scores_bleu),\n",
    "        corr_nice(scores_human, scores_bleu),\n",
    "        sep=\" | \"\n",
    "    )\n",
    "    print(\n",
    "        \"TER\",\n",
    "        corr_nice(scores_comet, scores_ter),\n",
    "        corr_nice(scores_human, scores_ter),\n",
    "        sep=\" | \"\n",
    "    )\n",
    "    print(\n",
    "        \"COMET-DA\",\n",
    "        corr_nice(scores_comet, scores_comet),\n",
    "        corr_nice(scores_human, scores_comet),\n",
    "        sep=\" | \"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "print_table(\"N1\", comet_n1, bleu_n1, chrf_n1, ter_n1, human)\n",
    "print_table(\"P1\", comet_p1, bleu_p1, chrf_p1, ter_p1, human)\n",
    "print_table(\"P3\", comet_p3, bleu_p3, chrf_p3, ter_p3, human)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import fig_utils\n",
    "\n",
    "\n",
    "def corr_nice(a, b):\n",
    "    return abs(kendalltau(a, b)[0])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 2.5))\n",
    "XTICKLABELS = [\"chrF\", \"BLEU\", \"TER\", \"COMET\"]\n",
    "def plot_corr(source_txt, offset_x, scores_comet, scores_bleu, scores_chrf, scores_ter, scores_human):\n",
    "    plt.xticks(range(len(XTICKLABELS)), XTICKLABELS)\n",
    "    plt.bar(\n",
    "       np.array(range(len(XTICKLABELS)))+offset_x,\n",
    "       [\n",
    "          corr_nice(scores_human, scores_chrf),\n",
    "          corr_nice(scores_human, scores_bleu),\n",
    "          corr_nice(scores_human, scores_ter),\n",
    "          corr_nice(scores_human, scores_comet),\n",
    "       ],\n",
    "       width=0.3,\n",
    "       label=source_txt\n",
    "    )\n",
    "    plt.ylim(None, 0.35)\n",
    "    plt.ylabel(\"$\\\\tau$ with Human\")\n",
    "    \n",
    "\n",
    "plot_corr(\"N1\", -0.3, bleu_n1, bleu_n1, chrf_n1, ter_n1, human)\n",
    "plot_corr(\"P1$\\\\star$\", -0.0, bleu_p1, bleu_p1, chrf_p1, ter_p1, human)\n",
    "plot_corr(\"P3\", +0.3, bleu_p3, bleu_p3, chrf_p3, ter_p3, human)\n",
    "\n",
    "plt.legend(\n",
    "    ncol=3, fancybox=False, edgecolor=\"black\",\n",
    "    handletextpad=0.1, columnspacing=1,\n",
    "    bbox_to_anchor=(1, 1.25),\n",
    "    loc=\"upper right\"\n",
    ").get_frame().set_linewidth(1.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure_single.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2.9))\n",
    "XTICKLABELS = [\"chrF\", \"BLEU\", \"TER\"]\n",
    "def plot_corr(source_txt, offset_x, scores_bleu, scores_chrf, scores_ter, scores_human):\n",
    "    plt.xticks(range(len(XTICKLABELS)), XTICKLABELS)\n",
    "    plt.bar(\n",
    "       np.array(range(len(XTICKLABELS)))+offset_x,\n",
    "       [\n",
    "          corr_nice(scores_human, scores_chrf),\n",
    "          corr_nice(scores_human, scores_bleu),\n",
    "          corr_nice(scores_human, scores_ter),\n",
    "       ],\n",
    "       width=0.2,\n",
    "       label=source_txt\n",
    "    )\n",
    "    plt.ylim(None, 0.35)\n",
    "    plt.ylabel(\"$\\\\rho$ with Human\")\n",
    "\n",
    "plot_corr(\"N1 P1$\\\\star$\", -0.3, bleu_n1_p1s, chrf_n1_p1s, ter_n1_p1s, human)\n",
    "plot_corr(\"N1 P1$\\\\star$ N1$\\\\star$\", -0.1, bleu_n1s_p1s, chrf_n1s_p1s, ter_n1s_p1s, human)\n",
    "plot_corr(\"N1 P1$\\\\star$ P3\", +0.1, bleu_n1_p1s_p3, chrf_n1_p1s_p3, ter_n1_p1s_p3, human)\n",
    "plot_corr(\"N1 P1$\\\\star$ P3 N1$\\\\star$ P3$\\\\star$\", +0.3, bleu_n1_p1s_p3_n1s_p3s, chrf_n1_p1s_p3_n1s_p3s, ter_n1_p1s_p3_n1s_p3s, human)\n",
    "\n",
    "plt.legend(\n",
    "    ncol=2, fancybox=False, edgecolor=\"black\",\n",
    "    handletextpad=0.1, columnspacing=1,\n",
    "    bbox_to_anchor=(1, 1.4),\n",
    "    loc=\"upper right\"\n",
    ").get_frame().set_linewidth(1.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure_mixed.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2.5))\n",
    "XTICKLABELS = [\"chrF\", \"BLEU\", \"TER\"]\n",
    "def plot_corr(source_txt, offset_x, scores_bleu, scores_chrf, scores_ter, scores_human):\n",
    "    plt.xticks(range(len(XTICKLABELS)), XTICKLABELS)\n",
    "    plt.bar(\n",
    "       np.array(range(len(XTICKLABELS)))+offset_x,\n",
    "       [\n",
    "          corr_nice(scores_human, scores_chrf),\n",
    "          corr_nice(scores_human, scores_bleu),\n",
    "          corr_nice(scores_human, scores_ter),\n",
    "       ],\n",
    "       width=0.3,\n",
    "       label=source_txt\n",
    "    )\n",
    "    plt.ylim(None, 0.35)\n",
    "    plt.ylabel(\"$\\\\rho$ with Human\")\n",
    "\n",
    "plot_corr(\"P1$\\\\star$ P3\", -0.3, bleu_p1s_p3, chrf_p1s_p3, ter_p1s_p3, human)\n",
    "plot_corr(\"N1 P1$\\\\star$\", +0.0, bleu_n1_p1s, chrf_n1_p1s, ter_n1_p1s, human)\n",
    "plot_corr(\"N1 P3\", +0.3, bleu_n1_p3, chrf_n1_p3, ter_n1_p3, human)\n",
    "\n",
    "plt.legend(\n",
    "    ncol=3, fancybox=False, edgecolor=\"black\",\n",
    "    handletextpad=0.1, columnspacing=1,\n",
    "    bbox_to_anchor=(1, 1.25),\n",
    "    loc=\"upper right\"\n",
    ").get_frame().set_linewidth(1.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure_mixed_different.pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "046b96a8882979e38267a32a551607f1ba06d2ceaf740f4fcb11894ced88ffd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
